{
  "paragraphs": [
    {
      "text": "%md\n## MySQL Docker Environment\nThis chapter\u0027s docker environment spins up `MySQL 8`.\nThe user `dataeng` has been created for working with MySQL data in Spark.\nThe password is customized in the `docker-compose-all.yaml` under `MYSQL_PASSWORD` (`dataengineering_user`). \n\nTo get into the MySQL console, use the following. When prompted use the password `dataengineering_user`\n~~~\ndocker exec -it mysql bash\nmysql -u dataeng -p\n~~~\n\nWhen you are logged in. You should see the following: \n~~~\nWelcome to the MySQL monitor.  Commands end with ; or \\g.\nYour MySQL connection id is 11\nServer version: 8.0.23 MySQL Community Server - GPL\n\nCopyright (c) 2000, 2021, Oracle and/or its affiliates.\n\nOracle is a registered trademark of Oracle Corporation and/or its\naffiliates. Other names may be trademarks of their respective\nowners.\n\nType \u0027help;\u0027 or \u0027\\h\u0027 for help. Type \u0027\\c\u0027 to clear the current input statement.\n\nmysql\u003e \n~~~\n\nWhen you are done `exit`\n",
      "user": "anonymous",
      "dateUpdated": "2021-02-14 22:38:59.601",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eMySQL Docker Environment\u003c/h2\u003e\n\u003cp\u003eThis chapter\u0026rsquo;s docker environment spins up \u003ccode\u003eMySQL 8\u003c/code\u003e.\u003cbr /\u003e\nThe user \u003ccode\u003edataeng\u003c/code\u003e has been created for working with MySQL data in Spark.\u003cbr /\u003e\nThe password is customized in the \u003ccode\u003edocker-compose-all.yaml\u003c/code\u003e under \u003ccode\u003eMYSQL_PASSWORD\u003c/code\u003e (\u003ccode\u003edataengineering_user\u003c/code\u003e).\u003c/p\u003e\n\u003cp\u003eTo get into the MySQL console, use the following. When prompted use the password \u003ccode\u003edataengineering_user\u003c/code\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edocker exec -it mysql bash\nmysql -u dataeng -p\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWhen you are logged in. You should see the following:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eWelcome to the MySQL monitor.  Commands end with ; or \\g.\nYour MySQL connection id is 11\nServer version: 8.0.23 MySQL Community Server - GPL\n\nCopyright (c) 2000, 2021, Oracle and/or its affiliates.\n\nOracle is a registered trademark of Oracle Corporation and/or its\naffiliates. Other names may be trademarks of their respective\nowners.\n\nType \u0027help;\u0027 or \u0027\\h\u0027 for help. Type \u0027\\c\u0027 to clear the current input statement.\n\nmysql\u0026gt; \n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWhen you are done \u003ccode\u003eexit\u003c/code\u003e\u003c/p\u003e\n\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1613341150711_1824638657",
      "id": "paragraph_1613341150711_1824638657",
      "dateCreated": "2021-02-14 22:19:10.711",
      "dateStarted": "2021-02-14 22:38:59.601",
      "dateFinished": "2021-02-14 22:38:59.616",
      "status": "FINISHED"
    },
    {
      "text": "%md\n## Create a Table and add some Data\n1. Reconnect to the Docker mysql container. `docker exec -it mysql bash` and run `mysql -u dataeng -p` and re-enter your password.\n2. Create the customers table\n3. Add some customers\n\n### Create the Customers Table\n~~~\nCREATE TABLE IF NOT EXISTS customers (\n  id VARCHAR(32),\n  created TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n  updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n  first_name VARCHAR(100),\n  last_name VARCHAR(100),\n  email VARCHAR(255)\n);\n~~~\n\n### Add Customers\n~~~\nINSERT INTO customers (id, first_name, last_name, email) VALUES (\"1\", \"Scott\", \"Haines\", \"scott@coffeeco.com\");\nINSERT INTO customers (id, first_name, last_name, email) VALUES (\"2\", \"John\", \"Hamm\", \"john.hamm@acme.com\");\nINSERT INTO customers (id, first_name, last_name, email) VALUES (\"3\", \"Milo\", \"Haines\", \"mhaines@coffeeco.com\");\n~~~",
      "user": "anonymous",
      "dateUpdated": "2021-02-15 01:16:56.876",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eCreate a Table and add some Data\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eReconnect to the Docker mysql container. \u003ccode\u003edocker exec -it mysql bash\u003c/code\u003e and run \u003ccode\u003emysql -u dataeng -p\u003c/code\u003e and re-enter your password.\u003c/li\u003e\n\u003cli\u003eCreate the customers table\u003c/li\u003e\n\u003cli\u003eAdd some customers\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3\u003eCreate the Customers Table\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003eCREATE TABLE IF NOT EXISTS customers (\n  id VARCHAR(32),\n  created TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n  updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n  first_name VARCHAR(100),\n  last_name VARCHAR(100),\n  email VARCHAR(255)\n);\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003eAdd Customers\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003eINSERT INTO customers (id, first_name, last_name, email) VALUES (\u0026quot;1\u0026quot;, \u0026quot;Scott\u0026quot;, \u0026quot;Haines\u0026quot;, \u0026quot;scott@coffeeco.com\u0026quot;);\nINSERT INTO customers (id, first_name, last_name, email) VALUES (\u0026quot;2\u0026quot;, \u0026quot;John\u0026quot;, \u0026quot;Hamm\u0026quot;, \u0026quot;john.hamm@acme.com\u0026quot;);\nINSERT INTO customers (id, first_name, last_name, email) VALUES (\u0026quot;3\u0026quot;, \u0026quot;Milo\u0026quot;, \u0026quot;Haines\u0026quot;, \u0026quot;mhaines@coffeeco.com\u0026quot;);\n\u003c/code\u003e\u003c/pre\u003e\n\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1613345119805_928436152",
      "id": "paragraph_1613345119805_928436152",
      "dateCreated": "2021-02-14 23:25:19.805",
      "dateStarted": "2021-02-15 01:16:56.870",
      "dateFinished": "2021-02-15 01:16:56.895",
      "status": "FINISHED"
    },
    {
      "text": "%md\n## Interoperating with MySQL and Apache Spark\nThe **Hive Metastore** can be used as a Data Catalog within the Spark ecosystem. This means that you can publish rich descriptions of the data your are producing, and this can be equally as useful when fetching data from other `tables` not managed by you or your team.\n\n### Hive MetaStore Configuration\n~~~\nspark.sql.hive.metastore.version: \"3.1.0\"\nspark.sql.hive.metastore.jars: maven\nspark.sql.hive.metastore.schema.verification: true\nspark.sql.hive.metastore.schema.verification.record.version: true\nspark.hadoop.javax.jdo.option.ConnectionURL jdbc:mysql://mysql:3306/default\nspark.hadoop.javax.jdo.option.ConnectionDriverName org.mariadb.jdbc.Driver\nspark.hadoop.javax.jdo.option.ConnectionUserName dataeng\nspark.hadoop.javax.jdo.option.ConnectionPassword dataengineering_user\n~~~",
      "user": "anonymous",
      "dateUpdated": "2021-02-14 22:32:40.428",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eInteroperating with MySQL and Apache Spark\u003c/h2\u003e\n\u003cp\u003eThe \u003cstrong\u003eHive Metastore\u003c/strong\u003e can be used as a Data Catalog within the Spark ecosystem. This means that you can publish rich descriptions of the data your are producing, and this can be equally as useful when fetching data from other \u003ccode\u003etables\u003c/code\u003e not managed by you or your team.\u003c/p\u003e\n\u003ch3\u003eHive MetaStore Configuration\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003espark.sql.hive.metastore.version: \u0026quot;3.1.0\u0026quot;\nspark.sql.hive.metastore.jars: maven\nspark.sql.hive.metastore.schema.verification: true\nspark.sql.hive.metastore.schema.verification.record.version: true\nspark.hadoop.javax.jdo.option.ConnectionURL jdbc:mysql://mysql:3306/default\nspark.hadoop.javax.jdo.option.ConnectionDriverName org.mariadb.jdbc.Driver\nspark.hadoop.javax.jdo.option.ConnectionUserName dataeng\nspark.hadoop.javax.jdo.option.ConnectionPassword dataengineering_user\n\u003c/code\u003e\u003c/pre\u003e\n\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1613341656118_1669951517",
      "id": "paragraph_1613341656118_1669951517",
      "dateCreated": "2021-02-14 22:27:36.118",
      "dateStarted": "2021-02-14 22:32:40.429",
      "dateFinished": "2021-02-14 22:32:40.447",
      "status": "FINISHED"
    },
    {
      "text": "%md\n## Spark Hive Jars\nYou will need to be working with a version of Spark that is built with all the right hive jars.\n\nRunning the following `shell` block will automatically scan and find the `hive` jars from your `SPARK_HOME`.",
      "user": "anonymous",
      "dateUpdated": "2021-02-14 22:36:25.419",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eSpark Hive Jars\u003c/h2\u003e\n\u003cp\u003eYou will need to be working with a version of Spark that is built with all the right hive jars.\u003c/p\u003e\n\u003cp\u003eRunning the following \u003ccode\u003eshell\u003c/code\u003e block will automatically scan and find the \u003ccode\u003ehive\u003c/code\u003e jars from your \u003ccode\u003eSPARK_HOME\u003c/code\u003e.\u003c/p\u003e\n\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1613342066987_2120126643",
      "id": "paragraph_1613342066987_2120126643",
      "dateCreated": "2021-02-14 22:34:26.988",
      "dateStarted": "2021-02-14 22:36:25.420",
      "dateFinished": "2021-02-14 22:36:25.431",
      "status": "FINISHED"
    },
    {
      "text": "%sh\nls -l ~/spark/jars | grep hive",
      "user": "anonymous",
      "dateUpdated": "2021-02-14 22:37:57.403",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "sh",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/sh"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "-rw-r--r-- 1 zeppelin root   183472 Aug 28 09:22 hive-beeline-2.3.7.jar\n-rw-r--r-- 1 zeppelin root    43387 Aug 28 09:22 hive-cli-2.3.7.jar\n-rw-r--r-- 1 zeppelin root   436980 Aug 28 09:22 hive-common-2.3.7.jar\n-rw-r--r-- 1 zeppelin root 10839104 Aug 28 09:22 hive-exec-2.3.7-core.jar\n-rw-r--r-- 1 zeppelin root   116311 Aug 28 09:22 hive-jdbc-2.3.7.jar\n-rw-r--r-- 1 zeppelin root   326419 Aug 28 09:22 hive-llap-common-2.3.7.jar\n-rw-r--r-- 1 zeppelin root  8194428 Aug 28 09:22 hive-metastore-2.3.7.jar\n-rw-r--r-- 1 zeppelin root   916206 Aug 28 09:22 hive-serde-2.3.7.jar\n-rw-r--r-- 1 zeppelin root    54116 Aug 28 09:22 hive-shims-0.23-2.3.7.jar\n-rw-r--r-- 1 zeppelin root     8785 Aug 28 09:22 hive-shims-2.3.7.jar\n-rw-r--r-- 1 zeppelin root   120305 Aug 28 09:22 hive-shims-common-2.3.7.jar\n-rw-r--r-- 1 zeppelin root    12984 Aug 28 09:22 hive-shims-scheduler-2.3.7.jar\n-rw-r--r-- 1 zeppelin root   234942 Aug 28 09:22 hive-storage-api-2.7.1.jar\n-rw-r--r-- 1 zeppelin root    38352 Aug 28 09:22 hive-vector-code-gen-2.3.7.jar\n-rw-r--r-- 1 zeppelin root   693141 Aug 28 09:22 spark-hive_2.12-3.0.1.jar\n-rw-r--r-- 1 zeppelin root  2079848 Aug 28 09:22 spark-hive-thriftserver_2.12-3.0.1.jar\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1613341144809_698892789",
      "id": "paragraph_1613341144809_698892789",
      "dateCreated": "2021-02-14 22:19:04.809",
      "dateStarted": "2021-02-14 22:37:57.414",
      "dateFinished": "2021-02-14 22:37:57.444",
      "status": "FINISHED"
    },
    {
      "text": "%spark\n\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.SparkContext\n\n// create a new SparkSession with hive support\nval sparkConfWithHive \u003d spark.sparkContext.getConf.clone.setAll(Map[String, String](\n  \"spark.sql.hive.metastore.version\" -\u003e \"3.1.0\",\n  \"spark.sql.hive.metastore.jars\" -\u003e \"maven\",\n  \"spark.sql.hive.metastore.schema.verification\" -\u003e \"true\",\n  \"spark.sql.hive.metastore.schema.verification.record.version\" -\u003e \"true\",\n  \"spark.hadoop.javax.jdo.option.ConnectionURL\" -\u003e \"jdbc:mysql://mysql:3306/default\",\n  \"spark.hadoop.javax.jdo.option.ConnectionDriverName\" -\u003e \"org.mariadb.jdbc.Driver\",\n  \"spark.hadoop.javax.jdo.option.ConnectionUserName\" -\u003e \"dataeng\",\n  \"spark.hadoop.javax.jdo.option.ConnectionPassword\" -\u003e \"dataengineering_user\"\n))\n\nval hiveSession \u003d SparkSession.builder\n  .appName(\"zeppelin-hive-sql\")\n  .config(sparkConfWithHive)\n  .enableHiveSupport()\n  .getOrCreate()\n\n",
      "user": "anonymous",
      "dateUpdated": "2021-02-15 00:04:33.212",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import org.apache.spark.sql.SparkSession\nimport org.apache.spark.SparkContext\n\u001b[1m\u001b[34msparkConfWithHive\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.SparkConf\u001b[0m \u003d org.apache.spark.SparkConf@4072b908\n\u001b[1m\u001b[34mhiveSession\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.SparkSession\u001b[0m \u003d org.apache.spark.sql.SparkSession@6aa3d7e\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1613342214308_1487474939",
      "id": "paragraph_1613342214308_1487474939",
      "dateCreated": "2021-02-14 22:36:54.309",
      "dateStarted": "2021-02-15 00:04:33.259",
      "dateFinished": "2021-02-15 00:04:57.414",
      "status": "FINISHED"
    },
    {
      "text": "%spark\nhiveSession.catalog.currentDatabase\njava.time.ZoneId.systemDefault\n//hiveSession.catalog.databaseExists(\"information_schema\")",
      "user": "anonymous",
      "dateUpdated": "2021-02-15 00:05:03.096",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mres1\u001b[0m: \u001b[1m\u001b[32mjava.time.ZoneId\u001b[0m \u003d Etc/UTC\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1613342412255_2121747230",
      "id": "paragraph_1613342412255_2121747230",
      "dateCreated": "2021-02-14 22:40:12.255",
      "dateStarted": "2021-02-15 00:05:03.123",
      "dateFinished": "2021-02-15 00:05:03.708",
      "status": "FINISHED"
    },
    {
      "title": "Connecting to MySQL",
      "text": "%spark\nval customers \u003d hiveSession.read\n  .format(\"jdbc\")\n  .option(\"url\", \"jdbc:mysql://mysql:3306/default\")\n  .option(\"driver\", \"org.mariadb.jdbc.Driver\")\n  .option(\"dbtable\", \"customers\")\n  .option(\"user\", \"dataeng\")\n  .option(\"password\", \"dataengineering_user\")\n  .load()\n\ncustomers.printSchema",
      "user": "anonymous",
      "dateUpdated": "2021-02-15 01:53:42.379",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "root\n |-- id: string (nullable \u003d true)\n |-- created: timestamp (nullable \u003d true)\n |-- updated: timestamp (nullable \u003d true)\n |-- first_name: string (nullable \u003d true)\n |-- last_name: string (nullable \u003d true)\n |-- email: string (nullable \u003d true)\n\n\u001b[1m\u001b[34mcustomers\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m \u003d [id: string, created: timestamp ... 4 more fields]\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1613353972468_2076014618",
      "id": "paragraph_1613353972468_2076014618",
      "dateCreated": "2021-02-15 01:52:52.469",
      "dateStarted": "2021-02-15 01:53:42.396",
      "dateFinished": "2021-02-15 01:53:42.732",
      "status": "FINISHED"
    },
    {
      "title": "Reading from MySQL",
      "text": "%spark\n\nimport java.sql.Timestamp\nimport org.apache.spark.sql.Dataset \nimport spark.implicits._\n\ncase class Customer(\n  id: String,\n  created: Timestamp,\n  first_name: String,\n  last_name: String,\n  email: String\n)\n\n// Datasets can come to the rescue as data changes behind the scenes. As long as you have a Data Contract in \n// place for specific columns that you need from each row in an external SQL table, then you can ignore new fields\n// that could otherwise cause problems with the runtime of your application code\n\nval customers: Dataset[Customer] \u003d hiveSession.read\n  .format(\"jdbc\")\n  .option(\"url\", \"jdbc:mysql://mysql:3306/default\")\n  .option(\"driver\", \"org.mariadb.jdbc.Driver\")\n  .option(\"dbtable\", \"customers\")\n  .option(\"user\", \"dataeng\")\n  .option(\"password\", \"dataengineering_user\")\n  .load()\n  .select($\"id\",$\"created\",$\"first_name\",$\"last_name\",$\"email\")\n  .as[Customer]\n\ncustomers.printSchema\ncustomers.show()",
      "user": "anonymous",
      "dateUpdated": "2021-02-15 01:51:55.398",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "root\n |-- id: string (nullable \u003d true)\n |-- created: timestamp (nullable \u003d true)\n |-- first_name: string (nullable \u003d true)\n |-- last_name: string (nullable \u003d true)\n |-- email: string (nullable \u003d true)\n\n+---+-------------------+----------+---------+--------------------+\n| id|            created|first_name|last_name|               email|\n+---+-------------------+----------+---------+--------------------+\n|  1|2021-02-15 01:27:10|     Scott|   Haines|  scott@coffeeco.com|\n|  2|2021-02-15 01:27:10|      John|     Hamm|  john.hamm@acme.com|\n|  3|2021-02-15 01:27:10|      Milo|   Haines|mhaines@coffeeco.com|\n+---+-------------------+----------+---------+--------------------+\n\nimport java.sql.Timestamp\nimport org.apache.spark.sql.Dataset\nimport spark.implicits._\ndefined class Customer\n\u001b[1m\u001b[34mcustomers\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.Dataset[Customer]\u001b[0m \u003d [id: string, created: timestamp ... 3 more fields]\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://zeppelin:4040/jobs/job?id\u003d2"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1613345100540_1141291465",
      "id": "paragraph_1613345100540_1141291465",
      "dateCreated": "2021-02-14 23:25:00.540",
      "dateStarted": "2021-02-15 01:51:55.412",
      "dateFinished": "2021-02-15 01:51:56.705",
      "status": "FINISHED"
    },
    {
      "text": "%spark\n// Customers purchase items from our Coffee Stores\n// For each Order, we encapsulate the total number of items purchased as an association to an order_id and a product_id\n// An order_id can be joined with a product_id to find the products being purchased by a Customer\n// Therefore, in order to keep track of what Customers purchase, we need to track only the customer_id, order_id and the timestamp (long, epoch millis)\nimport java.sql.Timestamp\ncase class Customer(\n  id: String,\n  created: Timestamp,\n  updated: Timestamp,\n  first_name: String,\n  last_name: String\n)\n\ncase class Item(\n  id: String,\n  name: String,\n  price: Int\n)\n\ncase class Order(\n  id: String,\n  items: Seq[Item],\n  total: Int\n)\n\ncase class CustomerOrder(\n  timestamp: Timestamp,\n  customer_id: String,\n  order_id: String\n)\n",
      "user": "anonymous",
      "dateUpdated": "2021-02-14 23:41:23.325",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import java.sql.Timestamp\ndefined class Customer\ndefined class Item\ndefined class Order\ndefined class CustomerOrder\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1613343031960_1273594006",
      "id": "paragraph_1613343031960_1273594006",
      "dateCreated": "2021-02-14 22:50:31.961",
      "dateStarted": "2021-02-14 23:36:07.034",
      "dateFinished": "2021-02-14 23:36:07.516",
      "status": "FINISHED"
    },
    {
      "text": "%spark\n",
      "user": "anonymous",
      "dateUpdated": "2021-02-14 22:51:41.671",
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1613343101671_1279721391",
      "id": "paragraph_1613343101671_1279721391",
      "dateCreated": "2021-02-14 22:51:41.671",
      "status": "READY"
    }
  ],
  "name": "ConnectingToMySQL",
  "id": "2FZWHD9DN",
  "defaultInterpreterGroup": "spark",
  "version": "0.9.0-preview2",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {},
  "config": {
    "isZeppelinNotebookCronEnable": false
  },
  "info": {}
}